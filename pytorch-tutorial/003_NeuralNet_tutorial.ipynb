{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "http://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "\n",
    "신경망은 torch.nn패키지를 사용해 구축가능하다.\n",
    "\n",
    "autograd를 체험해봤다면 모델을 구현하기위해 autograd를 이용해서 변수들을 미분한다. nn.Module은 레이어를 포함하고 forward(input)은 output을 리턴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망의 전형적인 학습 과정은 다음과 같다.\n",
    "\n",
    "* 학습가능한 파라미터를 가진 신경망을 정의한다.\n",
    "* 데이터셋을 반복적으로 입력한다.\n",
    "* 신경망을 통해 입력을 계산한다.\n",
    "* Loss를 계산한다(얼마나 output이 정확한지)\n",
    "* 신경망 파라미터를 backpropagation한다.\n",
    "* 신경망의 가중치를 업데이트한다. 일반적으로는 weight = weight - learning_rate * gradient 로 업데이트한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://pytorch.org/tutorials/_images/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망을 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 첫번째 입력은 1채널, 6 채널의 출력, 5x5 convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation : y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number.\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) # Conv1's weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0123 -0.0045 -0.0332 -0.0665  0.1261  0.0945  0.0469 -0.0184  0.0166 -0.0086\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ = Variable(torch.randn(1,1, 32, 32))\n",
    "out = net(input_)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 파라미터의 버퍼를 0으로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn은 오직 미니-배치 를 지원한다. 전체 torch.nn패키지는 샘플의 미니배치만을 입력으로 지원한다. 싱글 샘플말고.\n",
    "\n",
    "예를들어, nn.Conv2d는 4D 텐서를 입력받는다.\n",
    "nSamples * nChannels * Height(row) * Width(col).\n",
    "\n",
    "만약 싱글 샘풀을 갖고있다면, input.unsqueeze(0)을 써서 배치차원을 추가해주자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 넘어가기전에 보충을 한번 하자.\n",
    "\n",
    "* torch.Tensor - 다차원 array\n",
    "* autograd.Variable - 텐서를 감싸고 적용된 연산의 순서를 기억한다. Tensor와 같은 API를 갖고 backward()가 있다. \n",
    "* nn.Module - 신경망 모듈. GPU로 옮기는 헬퍼함수, 외부저장, 로딩 등.. 파라미터를 캡슐화하는 편한 방식이다. \n",
    "* nn.Parameter - Variable 처럼 Module의 attribute로 등록된 파라미터들\n",
    "* autograd.Function - autograd 연산의 전방/역방 정의의 구현.\n",
    "최소 싱글 Function노드를 만드는 모든 Variable객체의 연산은 Variable이 만든 함수로 연결되어있고 그 순서를 암호화한다.\n",
    "\n",
    "지금까지 우린 다음과 같은 사항을 진행해왔다.\n",
    "* 신경망을 구현\n",
    "* 입력을 처리하고 역전파 함수를 호출\n",
    "\n",
    "아직 남은 부분은..\n",
    "* loss 계산\n",
    "* 신경망 가중치 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "Loss function은 입력을 넣은 출력값과 타겟값(실제값) 쌍을 받고 얼마나 출력값이 타겟값과 차이가 있는지 계산한다.\n",
    "\n",
    "torch.nn 패키지에는 매우 다양한 loss function이 있다. \n",
    "간단한 Loss function은 nn.MSELoss 다. 출력과 타겟 간의 \n",
    "mean-squared error를 계산한다. \n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_n (Y_i - \\hat{Y_i})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 38.2924\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = net(input_)\n",
    "target = Variable(torch.arange(1, 11)) # 예를 들기위한 더미 타겟\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 38.2924\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((output - target)**2) / output.size()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "만약 당신이 .grad_fn attribute를 사용해 loss를 역방향으로 보내고 싶다면, 연산 그래프를 볼것이다.\n",
    "\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss\n",
    "\n",
    "loss.backward()를 호출할때 모든 그래프가 loss에 관해 미분된다.\n",
    "\n",
    "그리고 그래프의 모든 Variable객체는 .grad 변수에 gradient를 통합한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x00000293B6FAB358>\n",
      "<AddmmBackward object at 0x00000293B6FAB2E8>\n",
      "<ExpandBackward object at 0x00000293B6FAB358>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn) # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0]) # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop\n",
    "\n",
    "에러를 역전파시키기위해 해야할 것은 loss.backward()이다. \n",
    "\n",
    "존재하는 gradient들을 비워야 통합된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 6]\n",
      "\n",
      "conv1.bias.grad after backward\n",
      "Variable containing:\n",
      " 0.0563\n",
      "-0.0446\n",
      "-0.0238\n",
      "-0.1304\n",
      " 0.1107\n",
      "-0.0657\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad() # 모든 파라미터의 gradient버퍼를 0 초기화\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.0023\n",
       " 0.0000\n",
       " 0.0228\n",
       "-0.0221\n",
       "-0.0449\n",
       "-0.0343\n",
       " 0.0092\n",
       "-0.0575\n",
       " 0.2055\n",
       "-0.0704\n",
       "-0.1091\n",
       "-0.0243\n",
       "-0.0767\n",
       " 0.1045\n",
       "-0.0122\n",
       "-0.1219\n",
       "[torch.FloatTensor of size 16]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv2.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 신경망의 가중치를 업데이트 해보자.\n",
    "\n",
    "### Update the weights\n",
    "\n",
    "가장 단순한 업데이트 규칙은 Stochastic Gradient Descent 이다.\n",
    "\n",
    "$$W = weight, L = learning rate, G = gradient$$\n",
    "$$Weight = W - L * G$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금 당신은 신경망을 구현해보았지만, SGD, Nesterov-SGD, Adam, RMSProp, 등 다른 업데이트 규칙을 사용하고싶다면 torch.optim 패키지를 사용해보시라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# 트레이닝 루프에 추가\n",
    "optimizer.zero_grad()\n",
    "output = net(input_)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
